# -*- coding: utf-8 -*-
"""задание 14 1 "discrete control.ipynb"
Automatically generated by Colaboratory.
# Модель обучения с подкреплением в среде с дискретным управлением
Ниже находится несколько  упражнений, представляющих собой код с пропущенными фрагментами, которые надо воспроизвести. Часть из этих упражнений очень простые, над некоторыми надо хорошенько подумать.
Чтобы выполнить это задание, нужно сохранить копию файла себе на Google Диск.
После выполнения задания, его нужно отправить на проверку. Для этого достаточно предоставить доступ к файлу и отправить ссылку в соответствующее поле LMS курса.
Удачи в выполнении заданий!
## Дислеймер: задания этого урока носят в основном демонстрационный характер
Попробуйте внимательно прочитать код и комментарии к нему, запустить программу, понять, как она работает.
Далее попробовать сделать тоже самое но для другой среды с примерно таким же агентом.

# Рассмотрим простейший пример среды с дискретным пространством состояний среды
Рассмотрим среду [Taxi-v3](https://www.gymlibrary.dev/environments/toy_text/taxi/). Она представляет из себя парковку, разделенную на 5х5 мест. Есть четыре точки-остановки (R, G, Y, B), где можно забирать и высаживать пассажиров. За успешно доставленного пассажира начисляется 20 очков вознаграждения, за каждый ход снимается по 1 очку. На парковке вертикальными линиями отмечены стены-препятствия, которые нужно объезжать. Для этого у агента есть набор действий (ехать на Север, на Юг, на Запад, на Восток, подобрать пассажира, высадить пассажира). Пассажир находится в точке, буква которой синяя, фиолетовый звук буквы означает место пункта назначения. 

У среды есть 500 возможных состояний: 5х5 вариантов, где находится такси, пять возможных вариантов нахождения пассажира (4 точки и в машине) и 4 точки назначения.

Для демонстрации возможностей написанных функций ниже приведен код, создающий визуализацию среды. Однако агент в нем вместо действий по какому-либо осмысленному алгоритму совершает случайные действия.
"""

!pip install gym==0.19.0

import gym
env = gym.make("Taxi-v3")
observation = env.reset()

while True:
  
    env.render()
    
    # случайные действия агента
    action = env.action_space.sample() 
    #                    КОНЕЦ          
    observation, reward, done, info = env.step(action) #Применения действия к среде

    print(observation)    
    if done: 
      break
            
env.close()

"""Состояние системы можно закодировать 4 числами и даже вывести на экран. """

env.reset()
state = env.encode(2, 2, 3, 0) # (строка с такси, столбец с такси, местоположение пассажира, точка назначения)
print("Состояние:", state) 

env.env.s = state
env.render()

"""# Создадим Q-таблицу
У нас 6 действий и 500 состояний, соответственно таблица будет иметь размерность 500х6.
"""

import numpy as np
q_table = np.zeros([env.observation_space.n, env.action_space.n])

"""## Обучим модель """

import random
from IPython.display import clear_output

# Гиперпараметры
alpha = 0.1
gamma = 0.6
epsilon = 0.1

# Для метрик
all_epochs = []
all_penalties = []

for i in range(1, 100001):
    state = env.reset()

    epochs, penalties, reward, = 0, 0, 0
    done = False
    
    while not done:
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample() # Исследование
        else:
            action = np.argmax(q_table[state]) # Использование

        next_state, reward, done, info = env.step(action) 
        
        old_value = q_table[state, action] # подкручивание каэфицентов
        next_max = np.max(q_table[next_state]) # получили новое действие и 
        
        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
        q_table[state, action] = new_value

        if reward == -10:
            penalties += 1

        state = next_state
        epochs += 1
        
    if i % 100 == 0:
        clear_output(wait=True)
        print(f"Циклов обучения: {i}")

print("Обучение завершено.\n")

"""# После обучения проверим, как работает модель"""

state = env.reset()
done = False  
env.render()
while not done:
  action = np.argmax(q_table[state])
  state, reward, done, info = env.step(action)
  env.render()

"""## Упражнение
Теперь, когда принцип работы с дискретным пространством состояний, натренируйте похожую модель для среды [FrozenLake-v0](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/).
"""

import gym
env = gym.make('FrozenLake-v1', desc=[
    "SFFF",
    "FHFH",
    "FFFH",
    "HFFG"
    ], map_name="4x4", is_slippery=True)
observation = env.reset()
env.render() 

while True:
  
    env.render()
    
    # случайные действия агента
    action = env.action_space.sample() 
    #                    КОНЕЦ          
    observation, reward, done, info = env.step(action) #Применения действия к среде

    print(observation)    
    if done: 
      break
            
env.close()

import numpy as np
q_table = np.zeros([env.observation_space.n, env.action_space.n])

import random
from IPython.display import clear_output

# Гиперпараметры
alpha = 0.1
gamma = 0.6
epsilon = 0.1

# Для метрик
all_epochs = []
all_penalties = []

for i in range(1, 10):
    state = env.reset()

    epochs, penalties, reward, = 0, 0, 0
    done = False
    
    while not done:
        if random.uniform(0, 10000) < epsilon:
            action = env.action_space.sample() # Исследование
        else:
            action = np.argmax(q_table[state]) # Использование

        next_state, reward, done, info = env.step(action) 
        
        old_value = q_table[state, action] # подкручивание каэфицентов
        next_max = np.max(q_table[next_state]) # получили новое действие и 
        
        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
        q_table[state, action] = new_value

        if reward == -10:
            penalties += 1

        state = next_state
        epochs += 1
        
    if i % 100 == 0:
        clear_output(wait=True)
        print(f"Циклов обучения: {i}")

print("Обучение завершено.\n")

state = env.reset()
done = False  
env.render()
while not done:
  action = np.argmax(q_table[state])
  state, reward, done, info = env.step(action)
  env.render()