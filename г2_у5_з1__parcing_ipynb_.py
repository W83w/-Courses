# -*- coding: utf-8 -*-
"""г2 у5 з1 "parcing.ipynb"

Automatically generated by Colaboratory.

# Парсинг сайтов

## Инструкция по выполнению задания

Ниже находится несколько  упражнений, представляющих собой код с пропущенными фрагментами, которые надо воспроизвести. Часть из этих упражнений очень простые, над некоторыми надо хорошенько подумать. 

Чтобы выполнить это задание, нужно сохранить копию файла себе на Google Диск.
После выполнения задания, его нужно отправить на проверку. Для этого достаточно предоставить доступ к файлу и отправить ссылку в соответствующее поле LMS курса.
Удачи в выполнении заданий!

# Импортируем библиотеки
"""

import pandas as pd
import requests
import bs4

"""# Создадим запрос к сайту с прогнозами погоды




"""

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36"} # информация для сервера

r = requests.get('https://www.gismeteo.ru/catalog/russia/',  headers=headers) # запрос
print(r) # если ответ на запрос имеет код 200, то все хорошо
regions = bs4.BeautifulSoup(r.text) #создаем объект, где будет информация с сайта

"""## Упражнение 1
Найдите на сайте [Гисметео](https://www.gismeteo.ru/catalog/russia/), какие тэги отвечают за ссылки на регионы. Для этого зайдите в инструменты разработчика в браузере (чаще всего за это отвечает кнопка F12), найдите, при наведении на какой тэг мышкой, подсвечиваются ссылки на названия регионов.  
"""

regions.text

links = regions.find_all('a', {'class': "link-item"})
print(links)
print(type(links))

from tables import link
links[0].text

name_region = []
for i in range(len(links)):
  #print(links[i].text)
  name_region.append(links[i].text)
print(name_region)

"""## Упражнение 2
Методом [find_all()](https://docs-python.ru/packages/paket-beautifulsoup4-python/metody-find-all/) выберем все ссылки на регионы в отдельную переменную из переменной regions.  
"""

links = regions.find_all('a', {'class': "link-item"})
print(links)

"""## Упражнение 3
Методом [get('href')](https://www.tutorialspoint.com/how-can-beautifulsoup-be-used-to-extract-href-links-from-a-website) получим ссылки.  
"""

for link in regions.find_all('a'):
   print(link.get('href'))
print(len(link.get('href')))

"""## Упражнение 4
Сохраним названия регионов и ссылки в виде датафрейма.

[['Барнаул', '/'],
 ['Волгоград', '/'],
 ['Воронеж', '/news/'],
 ['Екатеринбург', '/maps/'],
 ['Казань', '/informers/'],
 ['Краснодар', '/soft/'],
 ['Красноярск', '/catalog/
"""

x = []
for i in regions.find_all('a'):
  if i == '/':
     pass
  else:
    x.append(link.get('href'))
print(x)
sort = ['/', '/', '/news/', '/maps/', '/informers/', '/soft/', '/catalog/']



for h in x:
  if h in sort:
    x.remove(h)

for h in x:
  if h != '/weather-barnaul-4720/':
    x.remove(h)
    print(h)
  elif h == '/weather-barnaul-4720/':
    break

  else:
    x.remove(h)

x

from re import split
region_df = []
b = 0 
for i in range(len(name_region)): 
  for j in range(len(x)):
    b+1
    if i == j:
      region_df.append([name_region[i], x[j]])

region_df

import pandas as pd
import numpy as np

columns = ['Регион', 'Ссылка']

values  = [region_df[0]]


table = pd.DataFrame(data = region_df, columns=columns)
table

"""## Упражнение 5
Сохраним датафрейм в виде файла.
"""

compression_opts = dict(method='zip',
                        archive_name='region.csv')  
table.to_csv('region.zip', index=False,
          compression=compression_opts)